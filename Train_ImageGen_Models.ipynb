{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "### 1) <a href='#section1'>Web Scraping and RSS Feed Parsing</a>\n",
    "### 2) <a href='#section2'>Data Cleaning and Feature Engineering</a>\n",
    "### 3) <a href='#section3'>Model Training</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter,defaultdict\n",
    "from operator import itemgetter\n",
    "import pickle\n",
    "import re\n",
    "import time,datetime\n",
    "\n",
    "import feedparser\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.doc2vec import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle functions for easy saving/loading\n",
    "def save_obj(obj,name):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping and RSS Feed Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load list containing all known authors\n",
    "all_authors = load_obj('all_authors_store')\n",
    "\n",
    "print(\"Current number of distinct authors: \" + str(len(all_authors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initiate selenium driver\n",
    "driver = webdriver.Chrome(\"chromedriver/chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to get tag links from page\n",
    "def get_tag_links(tag_links_set=set()):\n",
    "    for element in driver.find_elements_by_tag_name('a'):\n",
    "        tag_link = element.get_attribute('href')\n",
    "        if \"https://medium.com/tag\" in tag_link:\n",
    "            tag_links_set.add(tag_link)\n",
    "    return tag_links_set\n",
    "\n",
    "# method to get author links from page\n",
    "def get_new_authors(all_authors):\n",
    "    good_hrefs = []\n",
    "    for element in driver.find_elements_by_tag_name('a'):\n",
    "        href = element.get_attribute('href')\n",
    "        if \"@\" in href and \"?\" not in href:\n",
    "            good_hrefs.append(href)\n",
    "    \n",
    "    new_authors_set = set()\n",
    "    for author in good_hrefs:\n",
    "        new_authors_set.add(re.search('@([^/])+',author).group(0))\n",
    "\n",
    "    return all_authors | new_authors_set\n",
    "\n",
    "# method to scroll to bottom of page (so that all results appear)\n",
    "def scroll_to_bottom():\n",
    "    lenOfPage = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "    match=False\n",
    "    count = 0\n",
    "    while(match==False):\n",
    "            count += 1\n",
    "            lastCount = lenOfPage\n",
    "            time.sleep(3)\n",
    "            lenOfPage = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "            if lastCount==lenOfPage:\n",
    "                match=True\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author scraper approach 1 - spider across related tags\n",
    "# Takes a long time to run - see cell below for a more directed approach\n",
    "\n",
    "# replace \"technology\" with desired start tag\n",
    "driver.get('https://medium.com/tag/{}'.format('technology')) \n",
    "\n",
    "tag_links =  get_tag_links()\n",
    "new_tags = set()\n",
    "for tag in tag_links:\n",
    "    driver.get(tag)\n",
    "    new_tags = get_tag_links(tag_links_set=new_tags)\n",
    "tag_links = tag_links | new_tags\n",
    "\n",
    "for tag_link in tag_links:\n",
    "    scroll_to_bottom()\n",
    "    all_authors = get_new_authors(all_authors)\n",
    "\n",
    "save_obj(all_authors,'all_authors_backup2')\n",
    "\n",
    "print(\"New number of distinct authors: \" + str(len(all_authors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author scraper approach 2 - Directly search for specific keywords\n",
    "# Searching for a keyword yields more results for that keyword than the tag approach does\n",
    "\n",
    "#insert desired keywords \n",
    "search_keywords = [\"deep learning\",\"artificial intelligence\"]\n",
    "\n",
    "for keyword in search_keywords:\n",
    "    driver.get(\"https://medium.com/search?q={}\".format(keyword.replace(\" \",\"%20\")))\n",
    "    scroll_to_bottom()\n",
    "    all_authors = get_new_authors(all_authors)\n",
    "\n",
    "save_obj(all_authors,'all_authors_backup2')\n",
    "\n",
    "print(\"New number of distinct authors: \" + str(len(all_authors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# close driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load previously scraped articles for all authors\n",
    "old_titles = load_obj(\"old_titles\")\n",
    "for author in all_authors:\n",
    "    if author not in old_titles:\n",
    "        old_titles[author] = set()\n",
    "save_obj(old_titles,\"old_titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load previous medium csv\n",
    "medium_df = pd.read_csv(\"medium_posts.csv\")\n",
    "medium_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method to parse RSS feeds\n",
    "def get_posts(df, sources, rss_link):\n",
    "\n",
    "    old_titles = load_obj(\"old_titles\")\n",
    "    new_posts = {}\n",
    "    count = 1\n",
    "    \n",
    "    for source in sources:\n",
    "        feed = dict(feedparser.parse(rss_link.format(source)))\n",
    "        for post in feed['entries']:\n",
    "            title = post['title']\n",
    "            if title not in old_titles[source]:\n",
    "                old_titles[source].add(title)\n",
    "                try:\n",
    "                    new_posts[count] = {}\n",
    "                    new_posts[count]['datetime'] = str(datetime.datetime.now())\n",
    "                    new_posts[count]['url'] = post['link']\n",
    "                    new_posts[count]['id'] = post['id']\n",
    "                    new_posts[count]['title'] = title\n",
    "                    new_posts[count]['text'] = post['summary']\n",
    "                    new_posts[count]['author'] = post['author']\n",
    "                    new_posts[count]['published'] = post['published']\n",
    "                    new_posts[count]['published_parsed'] = post['published_parsed']\n",
    "\n",
    "                    keywords = ''\n",
    "                    for tag in post['tags']:\n",
    "                        keyword = tag['term']\n",
    "                        if keywords == '':\n",
    "                            keywords = keywords + keyword\n",
    "                        else:\n",
    "                            keywords = keywords + ' / ' + keyword \n",
    "                    new_posts[count]['keywords'] = keywords\n",
    "                    count += 1\n",
    "\n",
    "                except:\n",
    "                    count += 1\n",
    "                    continue\n",
    "                    \n",
    "    save_obj(old_titles, \"old_titles\")\n",
    "    \n",
    "    return pd.concat([df,pd.DataFrame.from_dict(new_posts,orient='index')],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run rss parser and save new csv (replaces previous one)\n",
    "# takes a few hours to run\n",
    "medium_rss_authors = 'https://medium.com/feed/@{}'\n",
    "medium_df = get_posts(medium_df,all_authors,medium_rss_authors)\n",
    "medium_df.to_csv('medium_posts.csv'.format(str(csv_count)), header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in csv containing all scraped data and metadata from posts \n",
    "medium_df = pd.read_csv('medium_posts12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create new column to store sequential post ids (will come in handy later)\n",
    "medium_df['article_id'] = np.arange(len(medium_df)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>datetime</th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "      <th>medium_url</th>\n",
       "      <th>published</th>\n",
       "      <th>published_parsed</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>post_id</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pierre GUILBAUD</td>\n",
       "      <td>2017-09-10 00:11:40.202998</td>\n",
       "      <td>https://medium.com/p/b1bdd7074234</td>\n",
       "      <td>startup / growth-hacking / email-marketing / e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon, 14 Aug 2017 07:06:33 GMT</td>\n",
       "      <td>time.struct_time(tm_year=2017, tm_mon=8, tm_md...</td>\n",
       "      <td>&lt;figure&gt;&lt;img alt=\"\" src=\"https://cdn-images-1....</td>\n",
       "      <td>Growth Hacking Workflow: 5 Steps to Go from Co...</td>\n",
       "      <td>https://medium.com/nookspot/growth-hacking-wor...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pierre GUILBAUD</td>\n",
       "      <td>2017-09-10 00:11:40.203076</td>\n",
       "      <td>https://medium.com/p/a0cc432bf0ac</td>\n",
       "      <td>startup / ideas / entrepreneur / network / pitch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wed, 05 Jul 2017 08:57:32 GMT</td>\n",
       "      <td>time.struct_time(tm_year=2017, tm_mon=7, tm_md...</td>\n",
       "      <td>&lt;figure&gt;&lt;img alt=\"\" src=\"https://cdn-images-1....</td>\n",
       "      <td>Pitch your startup in 30 Seconds</td>\n",
       "      <td>https://medium.com/@p.guilbaud/pitch-your-star...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pierre GUILBAUD</td>\n",
       "      <td>2017-09-10 00:11:40.203112</td>\n",
       "      <td>https://medium.com/p/57512d1cc8c</td>\n",
       "      <td>entrepreneurship / landing-pages / startup / b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue, 27 Jun 2017 10:53:08 GMT</td>\n",
       "      <td>time.struct_time(tm_year=2017, tm_mon=6, tm_md...</td>\n",
       "      <td>&lt;figure&gt;&lt;img alt=\"\" src=\"https://cdn-images-1....</td>\n",
       "      <td>Landing Page Optimization: Tools &amp; Tips</td>\n",
       "      <td>https://medium.com/@p.guilbaud/landing-page-op...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pierre GUILBAUD</td>\n",
       "      <td>2017-09-10 00:11:40.203145</td>\n",
       "      <td>https://medium.com/p/f94a35ca69aa</td>\n",
       "      <td>digital-strategy / social-media / growth-hacki...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri, 17 Mar 2017 08:15:55 GMT</td>\n",
       "      <td>time.struct_time(tm_year=2017, tm_mon=3, tm_md...</td>\n",
       "      <td>&lt;figure&gt;&lt;img alt=\"\" src=\"https://cdn-images-1....</td>\n",
       "      <td>5 Keys to Master your Digital Marketing Strategy</td>\n",
       "      <td>https://medium.com/@p.guilbaud/5-keys-to-maste...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pierre GUILBAUD</td>\n",
       "      <td>2017-09-10 00:11:40.203179</td>\n",
       "      <td>https://medium.com/p/59cfd24bc98b</td>\n",
       "      <td>startup / growth / entrepreneurship / accelera...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat, 11 Mar 2017 12:53:32 GMT</td>\n",
       "      <td>time.struct_time(tm_year=2017, tm_mon=3, tm_md...</td>\n",
       "      <td>&lt;figure&gt;&lt;img alt=\"\" src=\"https://cdn-images-1....</td>\n",
       "      <td>Five People Essential to your Startup Success</td>\n",
       "      <td>https://medium.com/@p.guilbaud/five-people-ess...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author                    datetime  \\\n",
       "0  Pierre GUILBAUD  2017-09-10 00:11:40.202998   \n",
       "1  Pierre GUILBAUD  2017-09-10 00:11:40.203076   \n",
       "2  Pierre GUILBAUD  2017-09-10 00:11:40.203112   \n",
       "3  Pierre GUILBAUD  2017-09-10 00:11:40.203145   \n",
       "4  Pierre GUILBAUD  2017-09-10 00:11:40.203179   \n",
       "\n",
       "                                  id  \\\n",
       "0  https://medium.com/p/b1bdd7074234   \n",
       "1  https://medium.com/p/a0cc432bf0ac   \n",
       "2   https://medium.com/p/57512d1cc8c   \n",
       "3  https://medium.com/p/f94a35ca69aa   \n",
       "4  https://medium.com/p/59cfd24bc98b   \n",
       "\n",
       "                                            keywords  medium_url  \\\n",
       "0  startup / growth-hacking / email-marketing / e...         NaN   \n",
       "1   startup / ideas / entrepreneur / network / pitch         NaN   \n",
       "2  entrepreneurship / landing-pages / startup / b...         NaN   \n",
       "3  digital-strategy / social-media / growth-hacki...         NaN   \n",
       "4  startup / growth / entrepreneurship / accelera...         NaN   \n",
       "\n",
       "                       published  \\\n",
       "0  Mon, 14 Aug 2017 07:06:33 GMT   \n",
       "1  Wed, 05 Jul 2017 08:57:32 GMT   \n",
       "2  Tue, 27 Jun 2017 10:53:08 GMT   \n",
       "3  Fri, 17 Mar 2017 08:15:55 GMT   \n",
       "4  Sat, 11 Mar 2017 12:53:32 GMT   \n",
       "\n",
       "                                    published_parsed  \\\n",
       "0  time.struct_time(tm_year=2017, tm_mon=8, tm_md...   \n",
       "1  time.struct_time(tm_year=2017, tm_mon=7, tm_md...   \n",
       "2  time.struct_time(tm_year=2017, tm_mon=6, tm_md...   \n",
       "3  time.struct_time(tm_year=2017, tm_mon=3, tm_md...   \n",
       "4  time.struct_time(tm_year=2017, tm_mon=3, tm_md...   \n",
       "\n",
       "                                                text  \\\n",
       "0  <figure><img alt=\"\" src=\"https://cdn-images-1....   \n",
       "1  <figure><img alt=\"\" src=\"https://cdn-images-1....   \n",
       "2  <figure><img alt=\"\" src=\"https://cdn-images-1....   \n",
       "3  <figure><img alt=\"\" src=\"https://cdn-images-1....   \n",
       "4  <figure><img alt=\"\" src=\"https://cdn-images-1....   \n",
       "\n",
       "                                               title  \\\n",
       "0  Growth Hacking Workflow: 5 Steps to Go from Co...   \n",
       "1                   Pitch your startup in 30 Seconds   \n",
       "2            Landing Page Optimization: Tools & Tips   \n",
       "3   5 Keys to Master your Digital Marketing Strategy   \n",
       "4      Five People Essential to your Startup Success   \n",
       "\n",
       "                                                 url  post_id  article_id  \n",
       "0  https://medium.com/nookspot/growth-hacking-wor...        1           1  \n",
       "1  https://medium.com/@p.guilbaud/pitch-your-star...        2           2  \n",
       "2  https://medium.com/@p.guilbaud/landing-page-op...        3           3  \n",
       "3  https://medium.com/@p.guilbaud/5-keys-to-maste...        4           4  \n",
       "4  https://medium.com/@p.guilbaud/five-people-ess...        5           5  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medium_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace NaN values with emptry string in keyword column\n",
    "medium_df.keywords.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dictionary to store keyword frequencies across all posts\n",
    "keyword_counts_dict = defaultdict(int)\n",
    "for i in medium_df.index:\n",
    "    keywords = medium_df.iloc[i]['keywords']\n",
    "    try:\n",
    "        for keyword in keywords.split(' / '):\n",
    "            keyword_counts_dict[keyword] += 1\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(keyword_counts_dict).most_common(200)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select posts based on relevant keywords and divide into 10 categories (to be used in web app)\n",
    "search_tech = ['tech ', 'technology', 'augmented-reality','android','apple', 'software-development', 'facebook'\n",
    "               'data-science', 'programming','machine-learning','artificial-intelligence','virtual-reality', 'vr',\n",
    "               'self-driving-cars','internet-of-things']\n",
    "search_politics = ['politics', 'government', 'donald-trump', 'trump', 'obama', 'hillary-clinton', 'russia',\n",
    "                   'republican-party','democrats', 'north-korea', 'congress','elections', 'democracy']\n",
    "search_entrepreneurship = ['entrepreneurship', 'startup', 'venture-capital', 'innovation', 'business','founders',\n",
    "                           'fundraising']\n",
    "search_economics = ['economics', 'finance', 'investing', 'money', 'stock-market', 'impact-investing', 'stocks',\n",
    "                    'banking','personal-finance', 'economy','investment']\n",
    "search_science = ['science', 'physics','space','climate-change','astronomy','neuroscience','nature','environment',\n",
    "                  'brain','nasa','evolution','biology','oceans','renewable-energy','eclipse','chemistry','ecology',\n",
    "                  'space-exploration','science-communication','geology']\n",
    "search_life = ['life', 'life-lessons', 'self-improvement', 'health', 'mental-health', 'life-hacking']\n",
    "search_education = ['education', 'teaching' 'learning', 'schools', 'higher-education','edtech', 'education-technology',\n",
    "                    'university', 'students','education-reform', 'school','teachers']\n",
    "search_writing = ['writing', 'books', 'poetry', 'fiction', 'storytelling', 'short-story']\n",
    "search_design = ['design-thinking','data-visualization', 'ux', 'ux-design', 'user-experience','product-design',\n",
    "                 'web-design', 'design-process', 'graphic-design']\n",
    "search_other = search_tech + search_politics + search_entrepreneurship + search_economics + search_life \\\n",
    "               + search_education + search_writing + search_design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create new data frames for each category\n",
    "tech_df = medium_df[medium_df['keywords'].str.contains('|'.join(search_tech))]\n",
    "politics_df = medium_df[medium_df['keywords'].str.contains('|'.join(search_politics))]\n",
    "entrepreneurship_df = medium_df[medium_df['keywords'].str.contains('|'.join(search_entrepreneurship))]\n",
    "economics_df = medium_df[medium_df['keywords'].str.contains('|'.join(search_economics))]\n",
    "science_df = medium_df[medium_df['keywords'].str.contains('|'.join(search_science))]\n",
    "life_df = medium_df[medium_df['keywords'].str.contains('|'.join(search_life))]\n",
    "education_df = medium_df[medium_df['keywords'].str.contains('|'.join(search_education))]\n",
    "writing_df = medium_df[medium_df['keywords'].str.contains('|'.join(search_writing))]\n",
    "design_df = medium_df[medium_df['keywords'].str.contains('|'.join(search_design))]\n",
    "other_df = medium_df[~medium_df['keywords'].str.contains('|'.join(search_other))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset indexes\n",
    "tech_df = pd.DataFrame.reset_index(tech_df)\n",
    "politics_df = pd.DataFrame.reset_index(politics_df)\n",
    "entrepreneurship_df = pd.DataFrame.reset_index(entrepreneurship_df)\n",
    "science_df = pd.DataFrame.reset_index(science_df)\n",
    "life_df = pd.DataFrame.reset_index(life_df)\n",
    "education_df = pd.DataFrame.reset_index(education_df)\n",
    "writing_df = pd.DataFrame.reset_index(writing_df)\n",
    "design_df = pd.DataFrame.reset_index(design_df)\n",
    "other_df = pd.DataFrame.reset_index(other_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_post(post):\n",
    "    # remove all tags besides <a> tags and non-header <p> tags\n",
    "    post = re.sub('<p><strong>.*?</strong></p>','',post)\n",
    "    post = re.sub('<h[0-9]>.*?</h[0-9]>','',post)        \n",
    "    post = re.sub('<figure>.*?</figure>','',post)         \n",
    "    post = re.sub('<iframe.*?</iframe>','',post)          \n",
    "    post = re.sub('<img.*?>','',post)                     \n",
    "    post = re.sub('<blockquote>.*?</blockquote>','',post) \n",
    "    post = re.sub('<ol>.*?</ol>','',post)                 \n",
    "    post = re.sub('<ul>.*?</ul>','',post)                 \n",
    "    post = re.sub('<em>|</em>','',post)                 \n",
    "    post = re.sub('<strong>|</strong>','',post) \n",
    "    post = re.sub('<pre>.*?</pre>','',post)               \n",
    "    post = re.sub('<hr>|<br>|<pr>','',post)               \n",
    "    return post\n",
    "\n",
    "def remove_a_tags(paragraph):\n",
    "    # remove <a> tags (but keep text within tag)\n",
    "    if \"</a>\" in paragraph:\n",
    "        start = paragraph.split(\"<a\")[0]\n",
    "        end = paragraph.split(\"</a>\")[-1]\n",
    "        no_a_tags = start + end\n",
    "        return re.sub('<.*?>','',no_a_tags)\n",
    "    else:\n",
    "        return paragraph\n",
    "    \n",
    "def remove_all_tags(paragraph):\n",
    "    return re.sub('<.*?>','',paragraph)\n",
    "  \n",
    "def return_paragaphs(post):\n",
    "    # return list of paragraphs, with all tags removed  \n",
    "    paragraphs = clean_post(post).split('</p>')\n",
    "    paragraphs_clean = [remove_a_tags(paragraph) for paragraph in paragraphs if paragraph != ''][:-1]\n",
    "    paragraph_list = [re.sub('<.*?>','',paragraph) for paragraph in paragraphs_clean]\n",
    "    if 'By' in paragraph_list[0]:\n",
    "        paragraph_list = paragraph_list[1:]\n",
    "    if 'recommend' or 'click' or 'originally published' in paragraph_list[-1].lower():\n",
    "        paragraph_list = paragraph_list[:-1]\n",
    "    return paragraph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_text(df,keyword):\n",
    "    # return a data frame containing all the non-tag text for each post \n",
    "    # and a dictionary mapping the original post ids to the new data frame index\n",
    "    good_texts = []\n",
    "    all_text_dict = {}\n",
    "    for i in df.index:\n",
    "        post = df.iloc[i]['text']\n",
    "        article_id = df.iloc[i]['article_id']\n",
    "        all_text = ''\n",
    "        try:\n",
    "            for paragraph in return_paragaphs(post):\n",
    "                all_text = all_text + ' ' + paragraph\n",
    "            all_text_dict[i] = {'all_text':all_text, 'article_id':article_id}\n",
    "            good_texts.append(article_id)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    all_text_df = pd.DataFrame.from_dict(all_text_dict,orient='index')\n",
    "    all_text_df = pd.DataFrame.reset_index(all_text_df)\n",
    "    all_text_df.rename(columns = {'index':'old_index'}, inplace = True)\n",
    "    \n",
    "    id_to_index = dict(zip(all_text_df.article_id,all_text_df.index))\n",
    "    save_obj(id_to_index,'{}_id_to_index'.format(keyword))\n",
    "    \n",
    "    return all_text_df, good_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run get_all_text() for all categories\n",
    "tech_post_df, tech_good_texts = get_all_text(tech_df,'tech')\n",
    "politics_post_df, politics_good_texts = get_all_text(politics_df,'politics')\n",
    "entrepreneurship_post_df, entrepreneurship_good_texts = get_all_text(entrepreneurship_df, 'entrepreneurship')\n",
    "science_post_df, science_good_texts = get_all_text(science_df,'science')\n",
    "life_post_df, life_good_texts = get_all_text(life_df,'life')\n",
    "education_post_df, education_good_texts = get_all_text(education_df,'education')\n",
    "writing_post_df, writing_good_texts = get_all_text(writing_df,'writing')\n",
    "design_post_df, design_good_texts = get_all_text(design_df,'design')\n",
    "other_post_df, other_good_texts = get_all_text(other_df,'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avoid_images(post):\n",
    "    # avoid images in last tenth of post (these are often icons unrelated to post content)\n",
    "    all_avoid = []\n",
    "    tenth_of_post = -int(len(post)/10)\n",
    "    for image_link in re.findall('src=\".*?\"',post[tenth_of_post:]):\n",
    "        all_avoid.append(image_link.replace('src=','').replace('\"',''))\n",
    "    return all_avoid\n",
    " \n",
    "def del_images_from_dict(img_para_dict,post):\n",
    "    # remove images from image --> paragraph dictionary \n",
    "    good_images = {}\n",
    "    for key,value in img_para_dict.items():\n",
    "        if value['img'] not in avoid_images(post):\n",
    "            good_images[key] = value\n",
    "    return good_images\n",
    "\n",
    "def get_img_w_paragraphs(post):\n",
    "    # return image --> paragraph dictionary\n",
    "    img_para_dict = {}\n",
    "    text_breaks = re.findall('<p>.*?</p><figure>.*?</figure>',post)\n",
    "    for i in range(len(text_breaks)):\n",
    "        img_w_para = text_breaks[i].split('<p>')[-1]\n",
    "        image = re.search('src=\".*?\"',img_w_para).group(0).replace('src=','').replace('\"','')\n",
    "        if 'http' in image:\n",
    "            paragraph_before = remove_all_tags(remove_a_tags(img_w_para))\n",
    "            two_paragraphs_before = remove_all_tags((remove_a_tags(text_breaks[i].split('<p>')[-2])))\n",
    "            img_para_dict[i] = {'img':image, 'para1':paragraph_before,\n",
    "                                'para2': two_paragraphs_before}\n",
    "    return del_images_from_dict(img_para_dict,post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_paragraphs_images(df,keyword,good_texts):\n",
    "    # return data frame containing paragraph-image pairs for each post \n",
    "    # and save three objects:  \n",
    "    # 1) image to paragraph dictionary \n",
    "    # 2) image to keyword dictionary \n",
    "    # 3) dictionary mapping the original post ids to the new data frame index \n",
    "    body_imgs_dict = {}\n",
    "    count = 0\n",
    "    for i in df.index:\n",
    "        article_id = df.iloc[i]['article_id']\n",
    "        if article_id in set(good_texts):\n",
    "            post = df.iloc[i]['text']\n",
    "            try:\n",
    "                keywords = df.iloc[i]['keywords']\n",
    "                img_list = get_img_w_paragraphs(post)\n",
    "                for key,value in img_list.items():\n",
    "                    img_link = value['img']\n",
    "                    para1 = value['para1'].lower()\n",
    "                    if 'para2' in value:\n",
    "                        para2 = value['para2'].lower()\n",
    "                    else:\n",
    "                        para2 = ''\n",
    "                    if 'len(para1.split(' ') + para2.split(' ')) > 15:\n",
    "                        body_imgs_dict[count] = {'image_link':img_link, '1_paragraph_before':para1, \n",
    "                                                 '2_paragraphs_before':para2, 'keywords':keywords, \n",
    "                                                 'article_id':article_id}\n",
    "                        count += 1\n",
    "            except:\n",
    "                continue\n",
    "         \n",
    "    body_imgs_df = pd.DataFrame.from_dict(body_imgs_dict,orient='index')\n",
    "    body_imgs_df = pd.DataFrame.reset_index(body_imgs_df)\n",
    "    body_imgs_df.rename(columns = {'index':'old_index'}, inplace = True)\n",
    "\n",
    "    img_to_para_dict = dict(zip(body_imgs_df.index, body_imgs_df.image_link))\n",
    "    img_to_kword_dict = dict(zip(body_imgs_df.index, body_imgs_df.keywords))\n",
    "    index_to_id = dict(zip(body_imgs_df.index, body_imgs_df.article_id))\n",
    "\n",
    "    save_obj(img_to_para_dict,'{}_img_to_para_dict'.format(keyword))\n",
    "    save_obj(img_to_kword_dict,'{}_img_to_kword_dict'.format(keyword))\n",
    "    save_obj(index_to_id,'{}_index_to_id'.format(keyword))\n",
    "\n",
    "    return body_imgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run get_paragraphs_images for all categories\n",
    "tech_paragraph_df = get_paragraphs_images(medium_df,'tech', tech_good_texts)\n",
    "politics_paragraph_df = get_paragraphs_images(medium_df,'politics', politics_good_texts)\n",
    "entrepreneurship_paragraph_df = get_paragraphs_images(medium_df, 'entrepreneurship', entrepreneurship_good_texts)\n",
    "science_paragraph_df = get_paragraphs_images(medium_df,'science', science_good_texts)\n",
    "life_paragraph_df = get_paragraphs_images(medium_df,'life', life_good_texts)\n",
    "education_paragraph_df = get_paragraphs_images(medium_df,'education', education_good_texts)\n",
    "writing_paragraph_df = get_paragraphs_images(medium_df,'writing', writing_good_texts)\n",
    "design_paragraph_df = get_paragraphs_images(medium_df,'design', design_good_texts)\n",
    "other_paragraph_df = get_paragraphs_images(medium_df,'other', other_good_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import stopword text file and save as list object \n",
    "with open(\"englishST.txt\",'r') as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "save_obj(stopwords,'stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemmatizer for getting standard dictionary forms of words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize(doc,lemmatizer):\n",
    "    # Lemmatize words in doc\n",
    "    lemma_doc = [lemmatizer.lemmatize(token) for token in doc]\n",
    "    return lemma_doc\n",
    "   \n",
    "def add_phrases(doc_list,keyword,para=True):\n",
    "    # Add bigrams and trigrams to docs (only those that appear 20 times or more)\n",
    "    bigram = Phrases(doc_list, min_count=20)\n",
    "    if para:\n",
    "        save_obj(bigram,'{}_para_bigram_list'.format(keyword))\n",
    "    else:\n",
    "        save_obj(bigram,'{}_post_bigram_list'.format(keyword))\n",
    "    \n",
    "    for idx in range(len(doc_list)):\n",
    "        for token in bigram[doc_list[idx]]:\n",
    "            if '_' in token:\n",
    "                # token is a bigram, add to document\n",
    "                doc_list[idx].append(token)\n",
    "    return doc_list\n",
    "\n",
    "def preprocess_lda(df,lemmatizer,keyword):\n",
    "    # preprocessing (remove stopwords,lemmatize,add phrases) for full post LDA\n",
    "    lda_posts = []\n",
    "    rows = df.iterrows()\n",
    "    for i in range(len(df)):\n",
    "        row = next(rows)[-1]\n",
    "        text = row['all_text']\n",
    "        processed = gensim.utils.simple_preprocess(text)\n",
    "        # remove stopwords\n",
    "        processed_stop = [word for word in processed if word not in stopwords]\n",
    "        # lemmatize\n",
    "        lemma_processed = lemmatize(processed_stop,lemmatizer)\n",
    "        lda_posts.append(lemma_processed)   \n",
    "    text_w_phrases = add_phrases(lda_posts,keyword,para=False)\n",
    "\n",
    "    return text_w_phrases\n",
    "\n",
    "def preprocess_paragraphs(df,lemmatizer,keyword):\n",
    "    # preprocessing (remove stopwords,lemmatize,add phrases) for paragraph LDA and doc2vec\n",
    "    lda_para = []\n",
    "    doc2vec_para = []\n",
    "    rows = df.iterrows()\n",
    "    for i in range(len(df)):\n",
    "        row = next(rows)[-1]\n",
    "        para1 = row['1_paragraph_before']\n",
    "        para2 = row['2_paragraphs_before']\n",
    "        index = i\n",
    "        processed = gensim.utils.simple_preprocess(para1)\n",
    "        if len(processed) < 10:\n",
    "            processed = processed + gensim.utils.simple_preprocess(para2)\n",
    "        # remove stopwords\n",
    "        processed_stop = [word for word in processed if word not in stopwords]\n",
    "        # lemmatize\n",
    "        lemma_processed = lemmatize(processed_stop,lemmatizer)\n",
    "        lda_para.append(lemma_processed)    \n",
    "        doc2vec_para.append(gensim.models.doc2vec.TaggedDocument(lemma_processed,[index]))         \n",
    "    text_w_phrases = add_phrases(lda_para,keyword)\n",
    "               \n",
    "    return text_w_phrases, doc2vec_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_lda_doc2vec(post_df,paragraph_df,keyword):\n",
    "    # train full post LDA model, pargraph LDA model, and paragraph doc2vec model \n",
    "\n",
    "    # lda on posts and paragraphs\n",
    "    all_text_w_phrases = preprocess_lda(post_df,lemmatizer,keyword)\n",
    "    paragraphs_w_phrases, doc2vec_paragraphs = preprocess_paragraphs(paragraph_df,lemmatizer,keyword)\n",
    "\n",
    "    dictionary_post = Dictionary(all_text_w_phrases)\n",
    "    dictionary_para = Dictionary(paragraphs_w_phrases)\n",
    "\n",
    "    # Filter out words that occur in more than 50% of the documents.\n",
    "    dictionary_post.filter_extremes(no_above=0.5)\n",
    "    dictionary_para.filter_extremes(no_above=0.5)\n",
    "\n",
    "    save_obj(dictionary_post,\"{}_lda_post_dict\".format(keyword))\n",
    "    save_obj(dictionary_para,\"{}_lda_para_dict\".format(keyword))\n",
    "\n",
    "    corpus_post = [dictionary_post.doc2bow(doc) for doc in all_text_w_phrases]\n",
    "    corpus_para = [dictionary_para.doc2bow(doc) for doc in paragraphs_w_phrases]\n",
    "\n",
    "    # Train LDA on full posts\n",
    "    \n",
    "    # Set training parameters\n",
    "    num_topics = 200\n",
    "    chunksize = 3000\n",
    "    passes = 3\n",
    "    iterations = 500\n",
    "    eval_every = None \n",
    "\n",
    "    # Make an index to word dictionary\n",
    "    temp = dictionary_post[0] \n",
    "    id2word = dictionary_post.id2token\n",
    "\n",
    "    lda_model = LdaModel(corpus=corpus_post, id2word=id2word, chunksize=chunksize, \\\n",
    "                           alpha='auto', eta='auto', \\\n",
    "                           iterations=iterations, num_topics=num_topics, \\\n",
    "                           passes=passes, eval_every=eval_every)\n",
    "\n",
    "    # Train LDA on paragraphs\n",
    "    \n",
    "    # Set training parameters\n",
    "    num_topics = 200\n",
    "    chunksize = 4000\n",
    "    passes = 5\n",
    "    iterations = 500\n",
    "    eval_every = None \n",
    "\n",
    "    # Make an index to word dictionary\n",
    "    temp = dictionary_para[0] \n",
    "    id2word = dictionary_para.id2token\n",
    "\n",
    "    lda_model_para = LdaModel(corpus=corpus_para, id2word=id2word, chunksize=chunksize, \\\n",
    "                           alpha='auto', eta='auto', \\\n",
    "                           iterations=iterations, num_topics=num_topics, \\\n",
    "                           passes=passes, eval_every=eval_every)\n",
    "\n",
    "\n",
    "    lda_model.save('{}_lda_model_posts'.format(keyword))\n",
    "    lda_model_para.save('{}_lda_model_para'.format(keyword))\n",
    "\n",
    "    lda_index = gensim.similarities.MatrixSimilarity(lda_model[corpus_post])\n",
    "    lda_index_para = gensim.similarities.MatrixSimilarity(lda_model_para[corpus_para])\n",
    "\n",
    "    save_obj(lda_index,'{}_lda_index_post'.format(keyword))\n",
    "    save_obj(lda_index_para,'{}_lda_index_para'.format(keyword))\n",
    "    \n",
    "    # Train doc2vec on paragraphs\n",
    "    doc2vec_model = Doc2Vec(iter=20,dm_concat=1)\n",
    "    doc2vec_model.build_vocab(doc2vec_paragraphs)\n",
    "    doc2vec_model.train(doc2vec_paragraphs, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.iter)\n",
    "\n",
    "    doc2vec_model.save('{}_doc2vec'.format(keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_list = [[tech_post_df, tech_paragraph_df, 'tech'],\n",
    "                 [politics_post_df, politics_paragraph_df, 'politics'],\n",
    "                 [entrepreneurship_post_df, entrepreneurship_paragraph_df, 'entrepreneurship'],\n",
    "                 [science_post_df, science_paragraph_df, 'science'],\n",
    "                 [life_post_df, life_paragraph_df, 'life'],\n",
    "                 [education_post_df,education_paragraph_df, 'education'],\n",
    "                 [writing_post_df,writing_paragraph_df,'writing'],\n",
    "                 [design_post_df,design_paragraph_df,'design'],\n",
    "                 [other_post_df, other_paragraph_df, 'other']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train models over all categories\n",
    "for category in category_list:\n",
    "    train_lda_doc2vec(category[0],category[1],category[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:insight]",
   "language": "python",
   "name": "conda-env-insight-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
